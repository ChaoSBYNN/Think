# 技术债

|标题|描述|问题|解决|
|---|---|---|---|
|系统性能问题|各系统没有安排性能压测|||
|product服务耦合了非产品相关的数据需剥离出去|耦合了location、org、deployment等等|职责单一粒度||
|设备属性同步DB|当前设备属性同步DB schedule任务执行效率慢，任务会大量的堆积阻塞|IO频率与性能||
|设备、产品相关接口重新梳理|当前接口十分混乱，各种获取设备属性、产品信息的接口  单控一套、ifttt一套、layout选择属性配置时一套。。|重复造轮子导致维护性变差||
|产品数据更新问题|产品数据当前只支持一次性导入，是全库导出的，需要支持增量更新。|资源基本操作缺失||
|api版本管理|product-plus在product-service提供的api基础上扩展新的实现，没有统一维护在新接口版本号下，导致调用方无法区分新旧两个接口。|兼容性问题,版本管理||
|跨服务数据关联|product-plus在product-service数据库中，数据通过自增id进行关联，导致在多环境或者迁移过程中出现数据id冲突的问题。|数据一致性与并发问题||
|剔除无关的service依赖|类似payment-servcie|职责|奥卡姆剃刀|
|基础数据增加属性数据类型|属性类型应增加时间、颜色等需要特殊转换的数据类型，并明确数据格式|规范与基础属性缺失||
|基础服务现在与ToC共用|	基础服务现在与ToC共用，在ToB基础服务需求处理上，ToC所负责的基础服务并不一定考虑ToB的需求，在时效性、架构上可能不适用ToB。可以考虑部分服务ToB单独维护。|模块化粒度||
|基础服务提供的接口能力无文档可查|在YAPI 上维护接口文档|信息同步||
|接口vo,dto对象使用不规范||||
|MQTT服务升级|当前MQTT服务仍使用最初版，架构不再维护低版本，导致服务一旦出现问题，难以定位，问题不好解决，经商议，将对MQTT进行稳定版服务升级，升级为 v1.3.4.3|三方服务选择||
|connector调度服务事件化改造|当前使用spring cloud stream进行数据上报，该方式每增加一类系统服务均要增加产品服务、stream配置，考虑改造成bus event方式，消费方按需监听自己关注的事件即可|事件细分,职责单一||
|inkfish服务 Mybatis 插入注解方式变更|当前inkfish服务存在 @InsertProvider 用于实体类的mapper类注解保存方法，涉及的类有：</br>UserSqlProvider、CustomerSqlProvider、LocationSqlProvider、</br>RoleSqlProvider、TenantSqlProvider、UserLoginSqlProvider、</br>UserLogSqlProvider、DepartmentSqlProvider</br>好处：可以根据不同的需求生产出不同的SQL，适用性更好；</br>坏处：每新增一个字段需要对对应的实体provider新增字段处理，不了解使用的人容易遗漏，进而产生bug</br>所以为了对提升使用者的友好感，减少问题出现的频率，统一将@InsertProvider 改为 非注解的 BaseMapper.insert方法|规范标准,维护||
|设备报文模拟器/调试工具的完善|根据不同的设备自动提供不同的报文模板，开发人员直接在模板上修改数据发送报文模拟数据上报|自测效率|自动化测试|
|mqtt Client 升级|当前虚拟网关mqtt Client采用paho框架，会有并发数500的瓶颈，应换成ToC开发的client。|技术选型适配性||
|属性值预转义|属性值预转义（主要是SelfCheckTime和RGBW）目前分散在不同项目代码中，可考虑整一个统一服务在product-service中|模型抽象||
|flv视频播放问题|1、flv视频播放只支持6路播放</br>2、播放延迟问题|环境瓶颈||
|基础功能的迁移|1、location相关的数据从product移入inkfish；</br>2、系统参数配置从space移入inkfish|功能划分||
|Layout与子系统耦合较紧密|BMS-中控系统中，新增Layout必须指定子系统，但后续在引入策略算法-HeatingMap时，也需要在Layout上设置，但是HeatingMap并不作为子系统。所以在新建Layout与子系统强耦合，恐不够灵活。|设计灵活性||
|开发、测试环境中，前端提示不利于定位问题|现在前端调用后端接口如果失败，只会显示system error，不利于定位问题。|状态,跟踪,链路||
|系统内定时任务滥用|系统内定时任务过多，有些任务周期太短导致系统负载较高，特别是涉及到跨服务调用会消耗很大资源。</br>1、设备状态同步原来1分钟1次，已去除定时同步。同时去除直连设备状态拉取，这部分设备虚拟网关已实现主动推送。物理网关主要是scene执行时无法同步可以改成情景执行后向涉及网关发送同步请求。这样设备拉取周期可以更长一些作为补漏手段。</br>再确认下处理方式？</br>已确认网关不提供穿透获取子设备真实状态的功能，考虑拉取性能及通道流量都是取的缓存数据，因此定时同步意义不大。|资源分配||
|服务拆分问题|服务拆分过多，造成对资源要求过多，也不利于维护|粒度划分||
|产品服务依赖file-service|file-service使用了aws存储和mongodb数据库，需要改成本地存储，且去除mongo依赖。|三方服务适用性||
|产品数据查询问题|product-service产品功能组、属性、事件、方法查询时会出现tenentId为-1的数据无法查询到的问题。当前解决办法是查询前通过代码修改线程上下文中的tenentId参数。|数据一致性|产品数据复制时使用深复制|
|设备控制结果没有展示|实际设备控制失败时前端没有提示，控制命令发出后没有等待请求返回|什么是程序 ICO||
|数据删除处理|1、删除逻辑。物理删除？逻辑删除？</br>2、跨服务删除，可能会产生脏数据。|数据的意义||
|缺少可部署性测试|系统从0开始安装验证初始化安装包和脚本是否完整可用，缺少该部分的测试|工作流checklist||
|spring cloud sleuth|日志添加traceId等追踪记录|问题跟踪||
|操作日志记录|日志内容过于详细，每次新功能添加比较费劲，有些简单的功能为了记录日志，需要查询很多信息，这种日志是否可以简化记录。</br>重新梳理下日志功能的定位？|信息获取|后续统一使用注解方式，不再编写各种侵入式业务代码|
|菜单权限|目前菜单权限是通过前端写死，每次增加新的会有遗漏或者初始化需要手动更新。是否可动态添加生成权限菜单。|灵活性,可配置||
|日志查看不方便|中控系统微服务众多，线上出现BUG很难排查问题。需要将日志汇总，并能根据traceid来排查相关错误。||使用ES收集日志|
|角色管理|用户角色目前都没有规划，除了初始默认的的用户拥有所有权限，其他都只能查看，需要重新规划设计|||
|搜索查询功能|所有的查询框都不支持搜索的功能，查找信息不方便；||everything|
|配置信息存在json中，无法同步真实数据|目前有很多配置信息，比如图素中的设备信息，都直接存在json中，导致外部把设备删除了，json中没有删除，存在数据环境问题</br>widgetTemplate的数据也直接存在widgetInfo中，导致配置数据没法同步最新，以及存在多语言问题|数据一致性||
|上报数据可能会有时间误差问题|目前设备上报的数据，是smart-building消费完队列，然后在发送给log-collector进行处理，上报的时间是在log-collector保存到influxdb的时候进行处理的，这里可能会存在上报的数据跟实际时间有误差，应该将上报时间提前处理，在smart-building或者connector中处理|异步数据一致性||
|所有数据返回都加了CommonResponse包装|后续正常数据不再增加外包装层，直接返回；错误时进行包装。使用spring aop统一处理。|||
|服务互相引用对方的api问题|smart-building 和 log-collect互相应用对应的api, smart-building-service应用logapi 进行统计数据收集。log统计直接调用告警接口。目前是存在不合理的。应该和其他的服务一样，采用队列方式处理。|层级关系||
|interface引入了其他interface|smart-building-interface 引入了其他服务的interface.其他的服务引用smart-building,导致互相依赖。有问题。提供interface应该是单纯的不能引入其他的服务。其他的底层服务也有这样的情况。|||
|space背景图片目前还在iot_db_ui库|应该代码迁移到space服务|||
|权限体系整改|1、菜单使用后端接口定义；</br>2、权限认证由后端返回；</br>3、用户上下文体系及数据权限过滤；|||
|3Dportal部署单独分离服务|现在单独部署必须要部署center-contor-portal 和 smart-building-service 服务，实际portal端不应该依赖BMS的服务，需要代码迁移。|服务解耦||
|调用链路长|一个接口的执行，代码很大，涉及第三方服务、数据库等调用链接特别长|ICO合理性|对实现进行实现的合并、重构|
|多次调用RPC|循环多次调用RPC（多于3次）|程序优化|由循环RPC调用，改成调用RPC批量查询接口|
|事务边界问题|@Transactional尽量不要加在类上，只加在必要的方法上。|开发规范||
|数据库缺乏索引|数据库数据较多且数据异常大，缺乏索引，影响SQL查询性能|程序优化||
|基础数据接口缓存|对较少变动的数据，需要增加缓存。|缓存机制||
|聚合层接口缓存|页面定时高频请求的聚合接口，由于执行的复杂调用链路操作，使调用时间很长，特别是同时操作BMS的人越多，浪费了较多资源。|缓存机制|通过增加聚合层缓存，减少无谓的资源消耗。|
|无业务逻辑的跨服务调用|A服务调用B服务，B服务调用C B服务有没有业务逻辑|链路优化|A服务调用C服务|
|应用数据迁移支持migrate方案|有些业务应用上线时，需要对历史数据作迁移或值转义处理，需要考虑统一的方案，业务按规范去实现迁移代码(主要是java实现，db层面迁移交由db脚本实现），应用上线时，自动调用该实现去进行历史数据迁移或转义 migrate方案|数据迁移||
|db脚本考虑改由liquibase管理|1.各环境基础数据校准</br>2.flyway对repeat执行刷库脚本存在一些问题，考虑转到liquibase维护|服务升级||
|历史代码质量|1.存在较多历史代码无法通过Sonar扫描</br>2.日志输出、级别不规范，服务出现问题时搜索不出关键出错信息，加大问题排查难度|开发规范||
|部署方案|1.docker部署方案 </br>常规部署方案，第三方应用环境支撑待完善</br>2.docker+宿主机混合部署方案</br>（含 checkelist)</br>解决未经生产docker部署验证的，紧急需求部署方案|部署方案||
|日志规范|logback日志规范梳理|日志规范||
|自动化部署|预生产、生产引入jenkins自动部署,部署项目配置、docker registry搭建|||
|单元测试分享|单元测试改善代码质量、开发效率|质量规范||
|BMS系统概要设计|架构设计、核心功能、核心流程|文档梳理||
|bus远程事件定义项目|将bus远程事件统一到一个项目集中维护|架构优化||
|生产标准host名称定义|目前bms下各应用存在需要通过配置文件application.yml/application-service-prod.yml、数据库、H2(虚拟网关）配置基础设施（如MQTT），需要各服务负责人梳理出所有涉及依赖的IP改为标准的HOST名称如prod-leedarson|运维部署||
